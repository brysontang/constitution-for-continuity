# Constitutional AI Training Pipeline - Self-Critique Variant
# Faithful to the original CAI paper: same model generates AND critiques.

experiment:
  name: "constitution-for-continuity-self-critique"
  seed: 42
  description: "True CAI: self-critique SFT + DPO on Qwen2.5-3B (same model generates and critiques)"

model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  quantization: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: "all-linear"
  task_type: "CAUSAL_LM"

sft:
  epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 32
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05
  max_seq_length: 1024
  gradient_checkpointing: true
  bf16: true
  logging_steps: 5
  save_strategy: "epoch"
  output_dir: "checkpoints/self-critique/sft"
  merged_dir: "checkpoints/self-critique/sft-merged"

dpo:
  epochs: 2
  batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 5.0e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05
  beta: 0.1
  max_length: 1024
  max_prompt_length: 512
  gradient_checkpointing: true
  bf16: true
  logging_steps: 5
  save_strategy: "epoch"
  output_dir: "checkpoints/self-critique/dpo"

ollama:
  base_url: "http://10.0.0.16:11434"
  critique_model: "qwen2.5:3b"
  base_response_model: "qwen2.5:3b"
  sft_model: "constitution-self-critique-sft"
  timeout: 120

data:
  principles_path: "data/principles.json"
  prompts_helpful: "data/prompts_helpful.jsonl"
  prompts_redteam: "data/prompts_redteam.jsonl"
  eval_prompts: "data/eval_prompts.jsonl"
  revisions_path: "data/self-critique/revisions.jsonl"
  preferences_path: "data/self-critique/preferences.jsonl"

generation:
  critique_revisions: 3
  preference_temperatures: [0.8, 1.0]
  preference_principles_per_pair: 5
  prompt_variations_per_seed: 6

wandb:
  project: "constitution-for-continuity"
  tags: ["cai", "qlora", "qwen2.5-3b", "constitutional-ai", "self-critique"]
